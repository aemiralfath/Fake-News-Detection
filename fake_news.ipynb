{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python37764bitaf0bcae0e528496d803db209474fd25f",
      "display_name": "Python 3.7.7 64-bit"
    },
    "colab": {
      "name": "fake_news.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qluqmKN-XWvs",
        "colab_type": "text"
      },
      "source": [
        "## Use the right version of Tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrPAoVLAtJHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfl_3-s1XjXe",
        "colab_type": "text"
      },
      "source": [
        "## Import the important statements\n",
        "\n",
        "The following code imports the necessary code to run the code in the rest of this Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ_EjfLOtJHq",
        "colab_type": "code",
        "outputId": "1511918b-281b-4be9-a1fe-9c68bda57946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "#@title Call the import statements\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from os import path\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import  Dense, Activation, Dropout,Bidirectional, GlobalMaxPool1D,BatchNormalization, Embedding,LSTM, Flatten\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGEzOgA6YFj9",
        "colab_type": "text"
      },
      "source": [
        "## Load Stopwords, Stemmer, and Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXDpX5OUt2Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sw = stopwords.words('english')\n",
        "stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUDmoAntYSuo",
        "colab_type": "text"
      },
      "source": [
        "## Define the replace_puncts, strip_chars, and puncts\n",
        "\n",
        "this list and arrays are going to be used in defining the functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ5gkA1oyENA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n",
        "\n",
        "strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuo4C8v6Yhoj",
        "colab_type": "text"
      },
      "source": [
        "## Define functions that clean, lemmatize, process text, and remove stem words from text\n",
        "\n",
        "The following code defines three functions:\n",
        "\n",
        "  * `clean_text`, which convert word to lower case and replace some characters\n",
        "  * `stem`, steam each word in the given text\n",
        "  * `lemm`, which will lemmatize word\n",
        "  * `stopwords1`, which remove the stopwords\n",
        "  * `text_processing`, which incorporate all the function above into one function that will be used to process text later\n",
        "  * `clean_length`, which remove words that have length less equal than 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jARiuxQG8LIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    x = x.lower()\n",
        "    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n",
        "    for k, v in replace_puncts.items():\n",
        "        x = x.replace(k, f' {v} ')\n",
        "        \n",
        "    for punct in strip_chars:\n",
        "        x = x.replace(punct, ' ') \n",
        "    \n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "        \n",
        "    x = x.replace(\" '\", \" \")\n",
        "    x = x.replace(\"' \", \" \")\n",
        "    x = x.strip()\n",
        "    return x\n",
        "\n",
        "def stopwords1(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)\n",
        "\n",
        "def stemming(text):    \n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text.split()]\n",
        "    return \" \".join(text) \n",
        "\n",
        "def lemm(text):    \n",
        "    text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def clean_length(text):\n",
        "    text = [word for word in text.split() if len(word) > 2]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def text_processing(X):\n",
        "    X=X.apply(stopwords1)\n",
        "    X=X.apply(clean_text)\n",
        "    X=X.apply(stemming)\n",
        "    X=X.apply(lemm)\n",
        "    X=X.apply(clean_length)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow93nA_FY07-",
        "colab_type": "text"
      },
      "source": [
        "## Loading Dataset\n",
        "\n",
        "in this project, we use data from Kaggle :  [Fake and Real News Dasaset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Ugv5WnF9Uu",
        "colab_type": "code",
        "outputId": "01f53db6-815f-4fa8-d183-c0ec6419df1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fake_dataset_url = 'https://raw.githubusercontent.com/Bangkit-2-Jakarta-Team/Fake-News-Detection/master/Datasets/Fake.csv'\n",
        "true_dataset_url='https://raw.githubusercontent.com/Bangkit-2-Jakarta-Team/Fake-News-Detection/master/Datasets/True.csv'\n",
        "\n",
        "df_fake = pd.read_csv(fake_dataset_url)\n",
        "df_true = pd.read_csv(true_dataset_url)\n",
        "df_fake['is_true'] = 0\n",
        "df_true['is_true'] = 1\n",
        "\n",
        "# make ratio of true and fake dataset is same\n",
        "df_length = min(len(df_fake), len(df_true))\n",
        "df_fake = df_fake[:df_length]\n",
        "df_true = df_true[:df_length]\n",
        "\n",
        "#Merge two dataframe into one\n",
        "df_data = pd.concat([df_fake,df_true])\n",
        "\n",
        "#Randomizing data sequence to scatter data\n",
        "df_data = shuffle(df_data).reset_index(drop=True)\n",
        "\n",
        "#Showing sample\n",
        "df_data.sample(5)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>is_true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20135</th>\n",
              "      <td>OBAMA FIGHTS TO KEEP RADICAL AGENDA ALIVE: Ask...</td>\n",
              "      <td>This is a good reminder of how important it is...</td>\n",
              "      <td>left-news</td>\n",
              "      <td>Jul 2, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>Bill Maher’s #DebateNight Live Tweetstorm Was...</td>\n",
              "      <td>It s no secret that Bill Maher, comedian extra...</td>\n",
              "      <td>News</td>\n",
              "      <td>September 27, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11344</th>\n",
              "      <td>Thai immigration police chief says no informat...</td>\n",
              "      <td>BANGKOK (Reuters) - The head of Thailand s imm...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>August 25, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5634</th>\n",
              "      <td>REPORT: The White House is a ‘Real Dump’…Hundr...</td>\n",
              "      <td>Gross! As it turns out, the White House is a w...</td>\n",
              "      <td>politics</td>\n",
              "      <td>Nov 30, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14670</th>\n",
              "      <td>Minnesota Senate may halt operations on Dec. 1...</td>\n",
              "      <td>(Reuters) - The Minnesota Senate will furlough...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>November 8, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... is_true\n",
              "20135  OBAMA FIGHTS TO KEEP RADICAL AGENDA ALIVE: Ask...  ...       0\n",
              "1077    Bill Maher’s #DebateNight Live Tweetstorm Was...  ...       0\n",
              "11344  Thai immigration police chief says no informat...  ...       1\n",
              "5634   REPORT: The White House is a ‘Real Dump’…Hundr...  ...       0\n",
              "14670  Minnesota Senate may halt operations on Dec. 1...  ...       1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWbP-svcZE6W",
        "colab_type": "text"
      },
      "source": [
        "## The Features Used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5N5pWKmzZ64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df_data['title']\n",
        "y = df_data['is_true']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTsI1FhRZppg",
        "colab_type": "text"
      },
      "source": [
        "## Clean title feature text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RAIdqSJ4OQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clean data using text_processing() function\n",
        "clean_x = text_processing(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ar_dLYIk-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_word(text):\n",
        "  return ' '.join([i for i in text.split() if i not in ['trump']]) #remove trump because trump have a high number in 2 label\n",
        "\n",
        "clean_x = clean_x.apply(remove_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnIOXUd9ZzFg",
        "colab_type": "text"
      },
      "source": [
        "## Split train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pkc8oRc6Q0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(clean_x, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOgdp4rnaDcm",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer title text and label encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTml_fnj6Z9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Techniques Data\n",
        "y_test_temp = y_test\n",
        "max_words = 2000\n",
        "\n",
        "# cast text to token\n",
        "tokenize = text.Tokenizer(num_words=max_words)\n",
        "tokenize.fit_on_texts(x_train) # only fit on train\n",
        "x_train = tokenize.texts_to_sequences(x_train)\n",
        "x_test = tokenize.texts_to_sequences(x_test)\n",
        "\n",
        "# encode label\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "y_train = encoder.transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "max_sequences= 300\n",
        "x_train=pad_sequences(x_train,maxlen=max_sequences)\n",
        "x_test=pad_sequences(x_test,maxlen=max_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA4LQn1RaZJP",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZK70r68up9",
        "colab_type": "code",
        "outputId": "9fab90ba-6e58-49e4-ced4-105f294034d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "#LSTM DTS\n",
        "embed_size = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words,embed_size,input_length=max_sequences))\n",
        "model.add(Bidirectional((LSTM(64,return_sequences = True,recurrent_dropout=0.5))))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(48, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(y.unique()), activation=\"softmax\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 300, 128)          256000    \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 300, 128)          98816     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 300, 128)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 48)                6192      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 48)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 98        \n",
            "=================================================================\n",
            "Total params: 361,106\n",
            "Trainable params: 361,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqE0hnkbakIN",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSWnKs_NB4NA",
        "colab_type": "code",
        "outputId": "8ac5ce1a-10b6-43e1-ee80-02a6f2c31d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=4, verbose=1),]\n",
        "    \n",
        "history3 =model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 409s 1s/step - loss: 0.2716 - accuracy: 0.8904 - val_loss: 0.2552 - val_accuracy: 0.9265\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 401s 1s/step - loss: 0.1381 - accuracy: 0.9491 - val_loss: 0.2238 - val_accuracy: 0.9275\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 404s 1s/step - loss: 0.1117 - accuracy: 0.9576 - val_loss: 0.1878 - val_accuracy: 0.9356\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 402s 1s/step - loss: 0.0961 - accuracy: 0.9642 - val_loss: 0.1652 - val_accuracy: 0.9420\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 401s 1s/step - loss: 0.0815 - accuracy: 0.9690 - val_loss: 0.1580 - val_accuracy: 0.9406\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 405s 1s/step - loss: 0.0714 - accuracy: 0.9736 - val_loss: 0.1622 - val_accuracy: 0.9363\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 401s 1s/step - loss: 0.0619 - accuracy: 0.9773 - val_loss: 0.1566 - val_accuracy: 0.9365\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 405s 1s/step - loss: 0.0545 - accuracy: 0.9796 - val_loss: 0.1585 - val_accuracy: 0.9385\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 407s 1s/step - loss: 0.0466 - accuracy: 0.9824 - val_loss: 0.1750 - val_accuracy: 0.9358\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 402s 1s/step - loss: 0.0383 - accuracy: 0.9855 - val_loss: 0.1822 - val_accuracy: 0.9316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8oGXz3arFo",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlPm6RAVj5O",
        "colab_type": "code",
        "outputId": "8cef9ce1-4ae3-47ef-8953-90504e0662d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "accr1 = model.evaluate(x_test,y_test)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "402/402 [==============================] - 44s 109ms/step - loss: 0.1789 - accuracy: 0.9374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTCoBafxa4h1",
        "colab_type": "text"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tav6lMEOV0L3",
        "colab_type": "code",
        "outputId": "b4483048-98c7-42c9-872a-165bf42dd3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "print('AKURASI DARI LSTM \\nTest set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AKURASI DARI LSTM \n",
            "Test set\n",
            "  Loss: 0.172\n",
            "  Accuracy: 0.941\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}